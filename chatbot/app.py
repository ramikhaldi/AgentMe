import sys
import os
from flask import Flask, request, jsonify
from langchain_community.llms import Ollama
from langchain.agents import initialize_agent, AgentType
from langchain_core.tools import Tool
from dotenv import load_dotenv
from custom_logic.tools import fibonacci_tool  # âœ… Import manually defined tool

# âœ… Ensure Python can find `custom_logic`
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

# Load environment variables
load_dotenv()
MODEL_NAME = os.getenv("MODEL_NAME", "llama3")
TTYD_API_PORT = os.getenv("TTYD_API_PORT", "5000")

app = Flask(__name__)

# âœ… 1ï¸âƒ£ Connect to Ollama (running in Docker network)
llm = Ollama(model=MODEL_NAME, base_url="http://ollama:11434")

# âœ… 2ï¸âƒ£ Manually register tools
tools = [fibonacci_tool]

# âœ… 3ï¸âƒ£ Use `initialize_agent` and ensure it stops after valid response
agent = initialize_agent(
    tools,
    llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # âœ… Standard tool-using agent
    verbose=True,
    max_iterations=2,  # âœ… Prevent unnecessary looping
    return_intermediate_steps=True,  # âœ… Ensure agent recognizes step completion
    handle_parsing_errors=True  # âœ… Prevents unexpected output errors
)

@app.route("/chat", methods=["POST"])
def chat():
    user_input = request.json.get("message", "")

    print(f"\nğŸ“¥ [USER QUESTION]: {user_input}")

    try:
        # âœ… Use `invoke()` instead of `run()`
        response = agent.invoke({"input": user_input})

        # âœ… Extract the correct final answer from `intermediate_steps`
        found_final_answer = False
        for i, step in enumerate(response["intermediate_steps"]):
            action, observation = step  # Each step is (AgentAction, Observation)

            # âœ… Print all intermediate steps for debugging
            print(f"\nğŸ”„ [INTERMEDIATE STEP {i+1}]")
            print(f"ğŸ› ï¸ Action: {action.tool}")
            print(f"ğŸ“¥ Input: {action.tool_input}")
            print(f"ğŸ‘€ Observation: {observation}")

            # âœ… Check for "Final Answer:"
            if "Final Answer:" in observation:
                final_result = observation.replace("Final Answer:", "").strip()
                print(f"\nğŸ¯ [FINAL RESULT (found in intermediate steps)]: {final_result}")
                found_final_answer = True
                return jsonify({"response": final_result})

        # âœ… If no final answer was found, print a warning and return output instead
        if not found_final_answer:
            print(f"\nâš ï¸ [WARNING]: No 'Final Answer' found in intermediate steps.")
            return jsonify({"response": response["output"]})

    except Exception as e:
        print(f"\nâŒ [ERROR]: {str(e)}")
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    print("ğŸš€ Chatbot is starting...")
    app.run(host="0.0.0.0", port=int(TTYD_API_PORT))
